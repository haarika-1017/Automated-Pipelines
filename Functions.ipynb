{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YVtMUT3FDyXx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from google.cloud import storage, bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "# Initialize clients for Google Cloud Storage and BigQuery\n",
        "storage_client = storage.Client()\n",
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "# Set the BigQuery dataset and table\n",
        "BQ_DATASET = 'sales_dataset'  # Replace with your dataset name\n",
        "BQ_TABLE = 'sales_table'      # Replace with your table name\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse the date and return it in a standard format (ISO 8601).\"\"\"\n",
        "    try:\n",
        "        return datetime.datetime.strptime(date_str, '%Y-%m-%d').isoformat()\n",
        "    except ValueError:\n",
        "        return None  # Return None for invalid dates\n",
        "\n",
        "def extract_data_from_gcs(bucket_name, file_name):\n",
        "    \"\"\"Extract the CSV file from Google Cloud Storage and load it into a pandas DataFrame.\"\"\"\n",
        "    # Read the CSV file from GCS\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    csv_data = blob.download_as_text()\n",
        "\n",
        "    # Parse the CSV data into a pandas DataFrame\n",
        "    df = pd.read_csv(io.StringIO(csv_data))\n",
        "    print(f\"Extracted {len(df)} rows from the file {file_name}.\")\n",
        "    return df\n",
        "\n",
        "def transform_data(df):\n",
        "    \"\"\"Perform transformations on the data: handle missing values, parse dates, etc.\"\"\"\n",
        "    # Handle missing values (fill with default values or drop rows)\n",
        "    df['quantity'].fillna(0, inplace=True)  # Fill missing quantities with 0\n",
        "    df['price'].fillna(0, inplace=True)  # Ensure no null prices\n",
        "\n",
        "    # Combine two columns into one (e.g., 'first_name' + 'last_name')\n",
        "    df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
        "\n",
        "    # Parse date fields and convert to a standard format\n",
        "    df['purchase_date'] = df['purchase_date'].apply(parse_date)\n",
        "\n",
        "    # Remove rows with invalid emails (e.g., not matching the email pattern)\n",
        "    df = df[df['email'].str.contains(r'^[^@]+@[^@]+\\.[^@]+$', na=False)]\n",
        "\n",
        "    # Create a new column for total cost (quantity * price)\n",
        "    df['total_cost'] = df['quantity'] * df['price']\n",
        "\n",
        "    print(\"Data transformation complete.\")\n",
        "    return df\n",
        "\n",
        "def load_data_to_bigquery(df, table_ref):\n",
        "    \"\"\"Load the transformed data into BigQuery using job configuration.\"\"\"\n",
        "    # Set up BigQuery job configuration with autodetect schema\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        autodetect=True,  # Auto-detect the schema\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Append data to the table\n",
        "        source_format=bigquery.SourceFormat.CSV  # Assuming CSV, modify if necessary\n",
        "    )\n",
        "\n",
        "    # Load data into BigQuery\n",
        "    try:\n",
        "        load_job = bigquery_client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "        load_job.result()  # Wait for the job to complete\n",
        "        print(f\"Successfully loaded {len(df)} rows into {table_ref}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load data to BigQuery: {e}\")\n",
        "\n",
        "def load_data_to_bigquery_trigger(data, context):\n",
        "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n",
        "       This function extracts, transforms, and loads data to BigQuery.\n",
        "    \"\"\"\n",
        "    bucket_name = data['bucket']\n",
        "    file_name = data['name']\n",
        "\n",
        "    # Log the event\n",
        "    print(f\"Triggered by file: gs://{bucket_name}/{file_name}\")\n",
        "\n",
        "    # Define the BigQuery table reference\n",
        "    table_ref = bigquery_client.dataset(BQ_DATASET).table(BQ_TABLE)\n",
        "\n",
        "    # 1. Extract: Get data from GCS\n",
        "    df = extract_data_from_gcs(bucket_name, file_name)\n",
        "\n",
        "    # 2. Transform: Apply transformations to the data\n",
        "    transformed_df = transform_data(df)\n",
        "\n",
        "    # 3. Load: Load the transformed data into BigQuery\n",
        "    load_data_to_bigquery(transformed_df, table_ref)\n"
      ]
    }
  ]
}